# -*- coding: utf-8 -*-
"""app1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pT4C9TfUpVHd3oRadn2OCE1epgaHUYVO
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import shap
import pickle
import sqlite3

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.svm import SVC, SVR
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score

# Set up Streamlit Tabs
st.set_page_config(page_title="AI-Assisted AutoML Tool", layout="wide")
tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“Š Data Upload & EDA", "ðŸ¤– Model Training", "ðŸ“ˆ Explainability", "ðŸ”® Real-Time Prediction"])

# ------------------- TAB 1: Data Upload & EDA -------------------
with tab1:
    st.title("ðŸ“Š Upload and Explore Your Data")
    uploaded_file = st.file_uploader("Upload CSV, Excel, or Fetch from Database", type=["csv", "xlsx"])
    
    if uploaded_file:
        df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
        st.write("### ðŸ” Preview of the Uploaded Data:")
        st.dataframe(df.head())

        # Handling Missing Values
        missing_values = df.isnull().sum()
        st.write("### â— Missing Values in Each Column:")
        st.write(missing_values[missing_values > 0])

        handle_missing = st.selectbox("How to handle missing values?", ["Mean Imputation", "Median Imputation", "Most Frequent", "Drop Rows", "Do Nothing"])
        if handle_missing == "Mean Imputation":
            imputer = SimpleImputer(strategy="mean")
        elif handle_missing == "Median Imputation":
            imputer = SimpleImputer(strategy="median")
        elif handle_missing == "Most Frequent":
            imputer = SimpleImputer(strategy="most_frequent")
        elif handle_missing == "Drop Rows":
            df.dropna(inplace=True)
        else:
            imputer = None
        
        if imputer:
            df[df.select_dtypes(include=np.number).columns] = imputer.fit_transform(df.select_dtypes(include=np.number))

        st.write("### âœ… Data after Handling Missing Values:")
        st.dataframe(df.head())

        # Basic EDA Visualization
        selected_column = st.selectbox("Select a Column for Visualization", df.columns)
        fig = px.histogram(df, x=selected_column)
        st.plotly_chart(fig)

# ------------------- TAB 2: Model Training -------------------
with tab2:
    st.title("ðŸ¤– Train a Machine Learning Model")
    
    if uploaded_file:
        target = st.selectbox("Select the Target Variable", df.columns)
        features = [col for col in df.columns if col != target]

        # Preprocess Data
        X = df[features]
        y = df[target]

        # Separate categorical and numerical features
        num_features = X.select_dtypes(include=np.number).columns.tolist()
        cat_features = X.select_dtypes(exclude=np.number).columns.tolist()

        # Preprocessing Pipelines
        num_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy="mean")),
            ('scaler', StandardScaler())
        ])
        cat_pipeline = Pipeline([
            ('encoder', OneHotEncoder(handle_unknown='ignore'))
        ])
        preprocessor = ColumnTransformer([
            ('num', num_pipeline, num_features),
            ('cat', cat_pipeline, cat_features)
        ])
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Model Selection
        model_type = st.radio("Select Task Type", ["Classification", "Regression"])
        model_options = {
            "Random Forest": RandomForestClassifier() if model_type == "Classification" else RandomForestRegressor(),
            "Decision Tree": DecisionTreeClassifier() if model_type == "Classification" else DecisionTreeRegressor(),
            "SVM": SVC() if model_type == "Classification" else SVR(),
            "Logistic Regression": LogisticRegression() if model_type == "Classification" else None
        }
        
        selected_model_name = st.selectbox("Choose a Model", list(model_options.keys()))
        selected_model = model_options[selected_model_name]

        # Hyperparameter Tuning
        if selected_model:
            param_grid = {
                "Random Forest": {"n_estimators": [50, 100, 200], "max_depth": [None, 10, 20]},
                "Decision Tree": {"max_depth": [None, 10, 20]},
                "SVM": {"C": [0.1, 1, 10]},
                "Logistic Regression": {"C": [0.1, 1, 10]}
            }
            grid = GridSearchCV(selected_model, param_grid[selected_model_name], cv=5)
            pipeline = Pipeline([('preprocessor', preprocessor), ('model', grid)])

            pipeline.fit(X_train, y_train)
            y_pred = pipeline.predict(X_test)

            st.write("### âœ… Model Performance:")
            if model_type == "Classification":
                acc = accuracy_score(y_test, y_pred)
                st.write(f"ðŸ“Š Accuracy: {acc:.2f}")
                st.text(classification_report(y_test, y_pred))
            else:
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                st.write(f"ðŸ“‰ Mean Squared Error: {mse:.2f}")
                st.write(f"ðŸ“Š R-Squared: {r2:.2f}")

            # Save Model
            with open("trained_model.pkl", "wb") as f:
                pickle.dump(pipeline, f)
            st.success("ðŸŽ‰ Model trained and saved!")

# ------------------- TAB 3: Model Explainability -------------------
with tab3:
    st.title("ðŸ“ˆ Model Explainability with SHAP")
    if uploaded_file:
        explainer = shap.Explainer(pipeline.named_steps['model'])
        shap_values = explainer(X_test)
        st.write("### Feature Importance:")
        st.pyplot(shap.plots.waterfall(shap_values[0]))

# ------------------- TAB 4: Real-Time Prediction -------------------
with tab4:
    st.title("ðŸ”® Real-Time Predictions")
    if uploaded_file:
        user_input = {}
        for col in features:
            user_input[col] = st.number_input(f"Enter {col}:", value=float(X[col].mean()))
        user_df = pd.DataFrame([user_input])
        
        with open("trained_model.pkl", "rb") as f:
            model = pickle.load(f)
        
        prediction = model.predict(user_df)
        st.write(f"### ðŸŽ¯ Predicted Value: {prediction[0]}")



