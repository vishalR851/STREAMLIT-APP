# -*- coding: utf-8 -*-
"""app1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pT4C9TfUpVHd3oRadn2OCE1epgaHUYVO
"""

!pip install streamlit

!pip install pyngrok

!pip install streamlit pyngrok pandas numpy matplotlib seaborn plotly scikit-learn

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score

st.title("Automated Data Analysis & ML Pipeline")

# 1. Data Upload
st.header("1. Upload Your Dataset")
uploaded_file = st.file_uploader("Upload CSV or Excel file", type=["csv", "xlsx"])
if uploaded_file:
    df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
    st.write("### 📌 Preview of the Uploaded Data:")
    st.dataframe(df.head())

    # 2. Data Cleaning & Preprocessing
    st.header("2. Data Cleaning & Preprocessing")
    if st.checkbox("Remove Duplicates"):
        df.drop_duplicates(inplace=True)
        st.write("✅ Duplicates removed!")

    missing_values = df.isnull().sum()
    st.write("### 🔍 Missing Values in Each Column:")
    st.write(missing_values[missing_values > 0])

    handle_missing = st.selectbox("How to handle missing values?", ["Mean Imputation", "Median Imputation", "Drop Rows", "Do Nothing"])
    if handle_missing == "Mean Imputation":
        df.fillna(df.mean(), inplace=True)
    elif handle_missing == "Median Imputation":
        df.fillna(df.median(), inplace=True)
    elif handle_missing == "Drop Rows":
        df.dropna(inplace=True)

    st.write("✅ Missing values handled!")

    # Outlier Detection
    if st.checkbox("Remove Outliers (Z-score > 3)"):
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df = df[(np.abs(df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std()) < 3].dropna()
        st.write("✅ Outliers removed!")

    # Data Type Conversion
    st.write("### Convert Data Types")
    for col in df.columns:
        if df[col].dtype == object:
            convert = st.radio(f"Convert column {col} to: ", ["Leave as is", "Convert to category", "Label Encode"], key=col)
            if convert == "Convert to category":
                df[col] = df[col].astype("category")
            elif convert == "Label Encode":
                df[col] = LabelEncoder().fit_transform(df[col])

    # Feature Scaling
    if st.checkbox("Apply Standard Scaling"):
        scaler = StandardScaler()
        df[df.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(df.select_dtypes(include=[np.number]))
        st.write("✅ Standard Scaling Applied!")

    st.write("### ✅ Data after Cleaning:")
    st.dataframe(df.head())

    # 3. Exploratory Data Analysis (EDA)
    st.header("3. Exploratory Data Analysis (EDA)")
    selected_column = st.selectbox("Select a Column for Visualization", df.columns)
    plot_type = st.selectbox("Choose Plot Type", ["Histogram", "Scatter", "Box Plot", "Correlation Matrix"])

    if plot_type == "Histogram":
        fig = px.histogram(df, x=selected_column)
    elif plot_type == "Scatter":
        fig = px.scatter(df, x=df.columns[0], y=selected_column)
    elif plot_type == "Box Plot":
        fig = px.box(df, y=selected_column)
    elif plot_type == "Correlation Matrix":
        fig, ax = plt.subplots()
        sns.heatmap(df.corr(), annot=True, cmap="coolwarm", ax=ax)
        st.pyplot(fig)

    st.plotly_chart(fig)

    # 4. Machine Learning Model Selection & Training
    st.header("4. Model Selection & Training")
    target = st.selectbox("Select the Target Variable", df.columns)
    features = st.multiselect("Select Feature Columns", [col for col in df.columns if col != target])

    X = df[features]
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Model Suggestion
    model_type = "Classification" if df[target].dtype == object or len(df[target].unique()) < 10 else "Regression"
    st.write(f"Suggested Task Type: {model_type}")

    model_options = {"Classification": ["Random Forest", "Logistic Regression", "SVM", "Decision Tree", "Naive Bayes"],
                     "Regression": ["Random Forest", "Linear Regression", "SVM", "Decision Tree"]}

    model_choice = st.selectbox("Select a Model", model_options[model_type])

    model = None
    if model_choice == "Random Forest":
        model = RandomForestClassifier() if model_type == "Classification" else RandomForestRegressor()
    elif model_choice == "Logistic Regression":
        model = LogisticRegression()
    elif model_choice == "SVM":
        model = SVC() if model_type == "Classification" else SVR()
    elif model_choice == "Decision Tree":
        model = DecisionTreeClassifier() if model_type == "Classification" else DecisionTreeRegressor()
    elif model_choice == "Naive Bayes":
        model = GaussianNB()

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Model Evaluation
    st.header("5. Model Evaluation")
    if model_type == "Classification":
        st.write(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
        st.text(classification_report(y_test, y_pred))
    else:
        st.write(f"Mean Squared Error: {mean_squared_error(y_test, y_pred):.2f}")
        st.write(f"R-Squared: {r2_score(y_test, y_pred):.2f}")

    # Predictions vs Actual Values
    st.header("6. Predictions vs Actual Values")
    fig, ax = plt.subplots()
    ax.scatter(y_test, y_pred, alpha=0.5)
    ax.set_xlabel("Actual Values")
    ax.set_ylabel("Predicted Values")
    ax.set_title("Predictions vs Actual Values")
    st.pyplot(fig)

!streamlit run app.py &>/dev/null &

import logging
import os
import signal # Import the signal module
from pyngrok import ngrok
# Disable pyngrok warnings
logging.getLogger("pyngrok").setLevel(logging.ERROR)

!ngrok authtoken 2u72IeZvUkS7VVpI32hd0zOWZH4_6CbaRrLLKayoy8dtQFZVs  # ✅ Set your token

# Kill any existing streamlit process
try:
    pid = int(os.popen("pgrep -f 'streamlit run app.py'").read())
    os.kill(pid, signal.SIGTERM) # Now signal is defined and can be used
except ValueError:
    pass

# Disconnect all existing tunnels
for tunnel in ngrok.get_tunnels():
    ngrok.disconnect(tunnel.public_url)

# Connect and get the public URL
public_url = ngrok.connect(8501).public_url
print(f"🚀 Public URL: {public_url}")

with open("requirements.txt", "w") as f:
    f.write("streamlit\npandas\nnumpy\nscikit-learn\nplotly")
from google.colab import files
files.download("requirements.txt")